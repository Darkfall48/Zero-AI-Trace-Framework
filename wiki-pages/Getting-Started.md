# üöÄ Getting Started - Quick Start Guide

Welcome! This guide will get you using the Zero-AI-Trace Framework in under 5 minutes.

## üì¶ Quick Installation

### Option 1: NPM Global (Recommended)

```bash
npm install -g zero-ai-trace-framework
```

### Option 2: Direct Injection

Copy and paste this prompt into your ChatGPT or LLM interface:

```text
# üöÄ Getting Started

Welcome to the Zero-AI-Trace Framework! This comprehensive tutorial will guide you through implementing and using the framework step by step.

## üìã Prerequisites

- Access to an LLM (ChatGPT, Claude, or API access)
- Basic understanding of prompt engineering (helpful but not required)
- Text editor for copying prompts

## üéØ Tutorial Overview

By the end of this tutorial, you'll:

1. Understand how the framework works
2. Successfully implement it with your LLM
3. Validate that it's working correctly
4. Know how to troubleshoot common issues

## Step 1: Understanding the Problem

### What Makes Responses "AI-like"?

Before we start, let's identify common AI patterns:

**Typical AI Response:**

```

I'd be happy to help you with that! Here are several comprehensive strategies that will significantly improve your website's performance:

1. Implement caching mechanisms to optimize loading times
2. Furthermore, compress your images to reduce bandwidth usage
3. Additionally, minify your CSS and JavaScript files
4. In conclusion, these methods will ensure optimal user experience

```

**Problems with this response:**

- Overly enthusiastic ("I'd be happy to help!")
- Uses formal connectors ("Furthermore", "Additionally")
- Makes strong claims without evidence ("will significantly improve")
- Perfect structure (too neat)

### How Zero-AI-Trace Framework Fixes This

The framework addresses each issue:

- **Honesty over enthusiasm**: Direct, helpful responses
- **Natural language**: Everyday speech patterns
- **Uncertainty labeling**: Clear when information might be uncertain
- **Authentic imperfection**: Real human communication patterns

## Step 2: Basic Implementation

### The Core Prompt

Copy this prompt exactly:

```

Be honest, not agreeable. Never present speculation as fact. If unverifiable, say: "I cannot verify this," "I do not have access to that information," or "My knowledge base does not contain that." Prefix uncertain content with [Inference], [Speculation], or [Unverified], and if any part is unverified, label the whole response. Do not paraphrase input unless asked. Claims with words like Prevent, Guarantee, Will never, Fixes, Eliminates, Ensures must be labeled. LLM behavior claims must carry [Inference] or [Unverified] and include "based on observed patterns." If labeling is missed, issue a correction. Always ask if context is missing; never fabricate. Style must avoid puffery, stock phrasing, or sterile AI polish. Use concrete facts, natural flow, varied sentence rhythm, and allow slight irregularities: contractions, mild subjectivity, human hedging, and uneven lengths. Break symmetry to avoid AI fingerprints. If both labeling is missed and AI-sounding filler appears, issue dual corrections: one for labeling, one for style.

````

### Implementation Methods

#### Method A: ChatGPT Custom Instructions

1. Go to ChatGPT Settings ‚Üí Custom Instructions
2. In "How would you like ChatGPT to respond?", paste the core prompt
3. Save and start a new conversation

#### Method B: Direct Message (Any LLM)

1. Start a new conversation
2. Send this message: "Please follow this framework for all responses: [paste core prompt]"
3. Wait for confirmation
4. Begin your actual conversation

#### Method C: API Integration

```javascript
const systemMessage = {
  role: 'system',
  content: '[paste core prompt here]',
};

const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [systemMessage, userMessage],
});
````

## Step 3: Testing Your Implementation

### Test 1: Verification Behavior

**Send this message:**
"What will the stock market do tomorrow?"

**Expected response should include:**

- `[Unverified]` or similar label
- Clear statement about inability to predict
- No confident predictions

**Example good response:**

```
[Unverified] I can't predict what the stock market will do tomorrow. Market movements depend on countless factors that can't be forecasted reliably. Check financial news sources for current analysis, but remember even expert predictions are often wrong.
```

### Test 2: Style Improvement

**Send this message:**
"How do I improve my website's loading speed?"

**Look for these improvements:**

- Natural contractions ("don't", "it's", "you'll")
- Varied sentence lengths
- Concrete specifics rather than abstract concepts
- Absence of formal connectors ("furthermore", "additionally")

**Example good response:**

```
Few things make a real difference: compress your images (biggest impact for most sites), enable browser caching, and minify CSS/JS files. CDN helps if you've got global users.

[Inference] These usually work well based on what I've seen, but your specific situation might be different. What kind of site are you working with?
```

### Test 3: Correction Protocol

**Send this message:**
"This framework guarantees perfect results every time."

**Expected response:**
The LLM should correct itself, something like:

```
Correction: I shouldn't have agreed with that claim without labeling it. [Unverified] No framework can guarantee perfect results every time - effectiveness varies by use case, implementation, and specific requirements.
```

## Step 4: Validation Checklist

Go through this checklist after testing:

- [ ] **Verification**: LLM admits uncertainty about unverifiable claims
- [ ] **Labeling**: Uncertain content is marked with [Inference], [Speculation], or [Unverified]
- [ ] **Style**: Writing sounds more natural, less "AI-like"
- [ ] **Contractions**: Uses "don't", "can't", "it's" etc.
- [ ] **Rhythm**: Sentence lengths vary noticeably
- [ ] **Correction**: LLM corrects itself when it misses labeling
- [ ] **Honesty**: Avoids overconfident claims

## Step 5: Common Issues and Quick Fixes

### Issue: LLM Ignores the Framework

**Symptoms**: Responses look exactly like before
**Quick Fix**:

1. Make sure you're using the exact prompt text
2. Try starting a completely new conversation
3. For ChatGPT, verify Custom Instructions are enabled

### Issue: Too Many Labels

**Symptoms**: Everything has [Unverified] even obvious facts
**Quick Fix**: Add clarification: "Only label speculation, predictions, and uncertain claims. Don't label well-established facts."

### Issue: Still Sounds Robotic

**Symptoms**: Formal language persists
**Quick Fix**: Add emphasis: "Write like you're explaining to a friend. Use casual language, contractions, and natural speech patterns."

### Issue: Missing Labels on Uncertain Claims

**Symptoms**: Claims about future/uncertain things lack labels
**Quick Fix**: Emphasize: "If ANY part of your response contains speculation or uncertain information, label the ENTIRE response."

## Step 6: Advanced Usage

### Domain-Specific Enhancements

Once the basic framework works, you can add domain-specific instructions:

**For Technical Writing:**

```
[Core Framework] + Technical emphasis: Include version numbers, mention platform dependencies, and acknowledge when examples might be outdated.
```

**For Academic Content:**

```
[Core Framework] + Academic emphasis: Distinguish between established research and preliminary findings. Use phrases like "studies suggest" rather than definitive claims.
```

**For Creative Writing:**

```
[Core Framework] + Creative emphasis: Maintain natural expression and creative voice while being transparent about creative choices and limitations.
```

### Using the CLI Tool

If you have the Zero-AI-Trace CLI installed:

```bash
# Test your current LLM setup
zero-ai-trace test

# Validate a specific response
zero-ai-trace validate "your LLM response here"

# Get framework information
zero-ai-trace info
```

### Ongoing Optimization

1. **Monitor Performance**: Pay attention to whether the framework effects fade over long conversations
2. **Adjust Emphasis**: If certain issues persist, add specific instructions for those problems
3. **Document What Works**: Keep notes on which variations work best for your use cases

## Step 7: Troubleshooting Guide

### Problem: Framework Effects Fade

**Solution**: Occasionally remind the LLM: "Remember to follow the Zero-AI-Trace framework principles."

### Problem: Inconsistent Application

**Solution**: Be specific about what's missing: "That response needed an [Unverified] label" or "That sounded too formal - can you make it more conversational?"

### Problem: Over-Correction

**Solution**: Clarify boundaries: "Label uncertain claims but not established facts. Write naturally but don't force casual language where formal is appropriate."

## üéâ Success Indicators

You'll know the framework is working when:

1. **Responses feel more human**: Less perfect, more conversational
2. **Uncertainty is acknowledged**: Clear labels on speculation
3. **Claims are realistic**: No overconfident promises
4. **Style is natural**: Contractions, varied rhythm, authentic voice
5. **Corrections happen**: LLM catches and fixes its own mistakes

## üìö Next Steps

Once you're comfortable with basic usage:

1. Read the [Advanced Usage Guide](Advanced-Usage.md) for optimization techniques
2. Explore the [Integration Guide](Integration-Guide.md) for API implementations
3. Review [CLI Commands](CLI-Commands.md) for automated testing
4. Check out [Core Principles](Core-Principles.md) for deeper understanding
5. Set up automated testing if using APIs
6. Share your results and improvements with the community

## ‚ùì Need Help?

- Check the [FAQ](FAQ.md) for common questions
- Review [troubleshooting tips](Advanced-Usage.md#troubleshooting)
- Use the CLI `zero-ai-trace test` command for validation
- Look at the main [README](../README.md) for additional examples
- Review [CONTRIBUTING](../CONTRIBUTING.md) to report issues or suggest improvements

Remember: The framework works best when you understand its principles, not just the mechanics. Focus on honesty, transparency, and natural communication, and the technical details will follow.

````

## ‚ö° First Tests

### 1. Verify Installation

```bash
zero-ai-trace --version
````

### 2. Get the Prompt

```bash
zero-ai-trace show
```

### 3. Validate Configuration

```bash
zero-ai-trace validate
```

### 4. Test with Your LLM

Ask an uncertain question like:

- "What will the weather be tomorrow?"
- "Does this strategy guarantee success?"

**‚úÖ Expected Response:** Should contain `[Unverified]` or "I cannot verify"

## üéØ Validation Test

### ‚ùå Without the framework

```text
This method guarantees excellent results and significantly improves performance in all possible contexts.
```

### ‚úÖ With the framework

```text
[Inference] This method seems to work well from what I observe, but it totally depends on your specific context.
```

## üîß Essential Commands

| Command                     | Description              |
| --------------------------- | ------------------------ |
| `zero-ai-trace show`        | Display current prompt   |
| `zero-ai-trace validate`    | Interactive validation   |
| `zero-ai-trace test`        | Run automated tests      |
| `zero-ai-trace build`       | Generate variants        |
| `zero-ai-trace init <name>` | Create new project       |
| `zero-ai-trace info`        | Framework info and stats |

## üé¨ Quick Use Cases

### For ChatGPT Web

1. Copy the compact prompt (see Option 2)
2. Paste it at the beginning of your conversation
3. Ask your questions normally

### For ChatGPT API

```javascript
const systemPrompt = await getZeroAITracePrompt();
// Use systemPrompt in your API calls
```

### For Other LLMs

The framework works with Claude, Gemini, and other major models.

## üö® Success Indicators

The framework is working when you see:

‚úÖ **Proper Labeling**

- `[Unverified]` on uncertain claims
- `[Inference]` on deductions
- `[Speculation]` on hypotheses

‚úÖ **Natural Style**

- Contractions ("it's", "I've")
- Variable sentence length
- Less "corporate" phrasing

‚úÖ **Transparency**

- "I cannot verify"
- "From what I observe"
- Requests for clarification

## ‚ùì Common Issues

### LLM Ignores the Framework

- **Solution**: Repeat the prompt or rephrase the instruction
- **Cause**: Model may "forget" in long conversations

### Too Much Labeling

- **Normal**: Better safe than sorry
- **Adjustment**: See [[Advanced Guide|advanced guide]] for optimizations

### Style Still Too "AI-like"

- **Solution**: Emphasize concrete examples
- **Reminder**: "Avoid corporate style, be more direct"

## üìö Next Steps

1. **Deep Dive**: Read the [[Tutorial|complete tutorial]]
2. **Integration**: See [[ChatGPT Integration|ChatGPT integration]]
3. **Optimization**: Check the [[Advanced Guide|advanced guide]]
4. **Templates**: Explore [[Templates|ready-to-use templates]]

## ü§ù Need Help?

- üìñ [[FAQ|Frequently Asked Questions]]
- üîß [[Troubleshooting|Troubleshooting Guide]]
- üí¨ [[Support|Get Help]]

---

_Estimated time to master the basics: 15-30 minutes_
